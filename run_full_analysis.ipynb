{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58a4b9d6",
   "metadata": {},
   "source": [
    "Código creado por Gonzalo Muñoz Olate @gmolate 2025 /// gonzalo.munoz@uchile.cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0044f08",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Check for required dependencies first/revisa los requisitos primero.\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234fd763",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def check_dependencies():\n",
    "    \"\"\"Check if required packages are installed\"\"\"\n",
    "    required_packages = [\n",
    "        'pandas', 'seaborn', 'matplotlib', 'sklearn', 'xgboost', \n",
    "        'imblearn', 'shap', 'numpy', 'scipy'\n",
    "    ]\n",
    "    \n",
    "    missing_packages = []\n",
    "    \n",
    "    for package in required_packages:\n",
    "        try:\n",
    "            if package == 'sklearn':\n",
    "                import sklearn\n",
    "            elif package == 'imblearn':\n",
    "                import imblearn\n",
    "            else:\n",
    "                __import__(package)\n",
    "        except ImportError:\n",
    "            missing_packages.append(package)\n",
    "    \n",
    "    if missing_packages:\n",
    "        print(\"ERROR: Faltan los siguientes paquetes de Python:\")\n",
    "        for pkg in missing_packages:\n",
    "            print(f\"  - {pkg}\")\n",
    "        print(\"\\nPara instalar todos los paquetes necesarios, ejecuta:\")\n",
    "        print(\"pip install pandas seaborn matplotlib scikit-learn xgboost imbalanced-learn shap numpy scipy statsmodels\")\n",
    "        print(\"\\nO si usas conda:\")\n",
    "        print(\"conda install pandas seaborn matplotlib scikit-learn xgboost imbalanced-learn shap numpy scipy statsmodels\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    print(\"✓ Todas las dependencias están instaladas correctamente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c9a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dependencies before importing anything else\n",
    "check_dependencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031057f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, f1_score\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import shap\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr, spearmanr, wilcoxon, ttest_rel\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091db76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make mcnemar optional (it's in statsmodels, not scipy)\n",
    "try:\n",
    "    from statsmodels.stats.contingency_tables import mcnemar\n",
    "    MCNEMAR_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MCNEMAR_AVAILABLE = False\n",
    "    # Simple McNemar implementation\n",
    "    def mcnemar_simple(table):\n",
    "        \"\"\"Simple McNemar test implementation\"\"\"\n",
    "        if table.shape != (2, 2):\n",
    "            return None, None\n",
    "        b = table.iloc[0, 1]  # off-diagonal elements\n",
    "        c = table.iloc[1, 0]\n",
    "        if b + c == 0:\n",
    "            return None, 1.0\n",
    "        # Chi-square with continuity correction\n",
    "        chi2 = (abs(b - c) - 1)**2 / (b + c)\n",
    "        from scipy.stats import chi2 as chi2_dist\n",
    "        p_value = 1 - chi2_dist.cdf(chi2, df=1)\n",
    "        return chi2, p_value\n",
    "    \n",
    "    print(\"Warning: statsmodels not available. Using simple McNemar implementation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d95e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84335a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuración Global ---\n",
    "EXCEL_FILE_INICIAL = 'Base datos inicial.xlsx'\n",
    "EXCEL_FILE_SEGUIMIENTO = 'Base datos seguimiento.xlsx'\n",
    "CSV_DIR = 'datos_csv'  # Directorio de CSVs limpios\n",
    "OUTPUT_DIR = 'analisis_resultados'  # Outputs (PNGs, logs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c714f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad45150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definición de Nombres de Columnas Limpios para cada Pestaña (INICIAL) ---\n",
    "# DIAGNOSTICO (7 columnas) - AHORA SIN 'funcionalidad_oral_final'\n",
    "clean_cols_diagnostico = [\n",
    "    'correlativo',\n",
    "    'periodontal_caso',\n",
    "    'periodontal_estado',\n",
    "    'periodontal_extension',\n",
    "    'periodontal_grado',\n",
    "    'implantes_presencia',\n",
    "    'implantes_estado'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e520108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRASPASO CUESTIONARIO (66 columnas)\n",
    "clean_cols_traspaso = [\n",
    "    'correlativo', 'edad', 'sexo', 'uso_ges_60', 'nivel_educacional', 'alcanza_dinero_fin_mes',\n",
    "    'resultado_empam', 'diabetes', 'hba1c', 'dislipidemia', 'hipertension', 'otra_enfermedad',\n",
    "    'otra_enfermedad_cual', 'tto_cancer_cyc', 'usa_farmacos', 'farmacos_cuales', 'alergias',\n",
    "    'es_fumador', 'cigarros_por_dia', 'consume_alcohol', 'frecuencia_alcohol', 'consume_drogas',\n",
    "    'frecuencia_drogas', 'peso_kg', 'altura_cm', 'contenido_dieta_cariogenica', 'frecuencia_dieta',\n",
    "    'consumo_citricos', 'consumo_bebidas_carbonatadas', 'consumo_vino', 'consumo_sidra',\n",
    "    'boca_seca_al_comer', 'necesita_liquidos_tragar', 'dificultad_tragar_general', 'cantidad_saliva',\n",
    "    'protesis_maxilar', 'protesis_maxilar_extension', 'protesis_mandibular', 'protesis_mandibular_extension',\n",
    "    'protesis_mandibular_material', 'satisfecho_protesis_sup', 'motivo_insatisfaccion_sup',\n",
    "    'satisfecho_protesis_inf', 'motivo_insatisfaccion_inf', 'n_protesis_sup_hechas', 'n_protesis_inf_hechas',\n",
    "    'usa_pasta_5000ppm', 'usa_colutorios', 'usa_pasta_1000ppm',\n",
    "    'ttm_dolor_mandibula_4sem', 'ttm_dolor_masticar_4sem', 'ttm_dolor_abrir_boca_4sem',\n",
    "    'ttm_mandibula_trabada_4sem', 'ttm_ruidos_articulacion_4sem', 'ttm_requiere_derivacion',\n",
    "    'eat10_perdida_peso', 'eat10_comer_fuera', 'eat10_tragar_liquidos', 'eat10_tragar_solidos',\n",
    "    'eat10_tragar_pastillas', 'eat10_tragar_doloroso', 'eat10_placer_comer', 'eat10_comida_pegada',\n",
    "    'eat10_tos_al_comer', 'eat10_tragar_estresante', 'eat10_requiere_derivacion'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a1406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALIDAD DE VIDA BASAL (33 columnas)\n",
    "clean_cols_calidad_vida = [\n",
    "    'correlativo',\n",
    "    'gohai_q1', 'gohai_q2', 'gohai_q3', 'gohai_q4', 'gohai_q5', 'gohai_q6',\n",
    "    'gohai_q7', 'gohai_q8', 'gohai_q9', 'gohai_q10', 'gohai_q11', 'gohai_q12',\n",
    "    'ohip_q1', 'ohip_q2', 'ohip_q3', 'ohip_q4', 'ohip_q5', 'ohip_q6',\n",
    "    'ohip_q7', 'ohip_q8', 'ohip_q9', 'ohip_q10', 'ohip_q11', 'ohip_q12',\n",
    "    'ohip_q13', 'ohip_q14',\n",
    "    'eq5d_movilidad', 'eq5d_cuidado_personal', 'eq5d_actividades_habituales',\n",
    "    'eq5d_dolor_malestar', 'eq5d_angustia_depresion', 'eq5d_score_paciente'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d627ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCIONALIDAD ORAL (12 columnas)\n",
    "clean_cols_funcionalidad_oral = [\n",
    "    'correlativo',\n",
    "    'leake_q1', 'leake_q2', 'leake_q3', 'leake_q4', 'leake_q5',\n",
    "    'leake_resultado', 'eichner', 'funcion_masticatoria',\n",
    "    'fuerza_oclusal', 'diadococinesia', 'funcion_deglutoria'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LESIONES MUCOSA ORAL (4 columnas)\n",
    "clean_cols_lesiones_mucosa_oral = [\n",
    "    'correlativo',\n",
    "    'lesion_ubicacion',\n",
    "    'lesion_elemental',\n",
    "    'lesion_nombre_texto'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91180c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COPD (33 columnas)\n",
    "clean_cols_copd = [\n",
    "    'correlativo',\n",
    "    'diente_18', 'diente_17', 'diente_16', 'diente_15', 'diente_14', 'diente_13', 'diente_12', 'diente_11',\n",
    "    'diente_21', 'diente_22', 'diente_23', 'diente_24', 'diente_25', 'diente_26', 'diente_27', 'diente_28',\n",
    "    'diente_48', 'diente_47', 'diente_46', 'diente_45', 'diente_44', 'diente_43', 'diente_42', 'diente_41',\n",
    "    'diente_31', 'diente_32', 'diente_33', 'diente_34', 'diente_35', 'diente_36', 'diente_37', 'diente_38'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b5d7f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- Definición de Nombres de Columnas Limpios para Traspaso Cuestionario (65 columnas de seguimiento) ---\n",
    "clean_cols_traspaso_seguimiento = [\n",
    "    'correlativo', 'edad', 'sexo', 'tiempo_seguimiento_alta', 'motivo_no_termino_tto',\n",
    "    'nivel_funcionalidad_empam', 'diabetes', 'hba1c', 'dislipidemia', 'hipertension', 'otra_enfermedad',\n",
    "    'otra_enfermedad_cual', 'tto_cancer_cyc', 'usa_farmacos', 'farmacos_cuales', 'alergias',\n",
    "    'es_fumador', 'cigarros_por_dia', 'consume_alcohol', 'frecuencia_alcohol', 'consume_drogas',\n",
    "    'frecuencia_drogas', 'peso_kg', 'altura_cm', 'contenido_dieta_cariogenica', 'frecuencia_dieta',\n",
    "    'consumo_citricos', 'consumo_bebidas_carbonatadas', 'consumo_vino', 'consumo_sidra',\n",
    "    'boca_seca_al_comer', 'necesita_liquidos_tragar', 'dificultad_tragar_general', 'cantidad_saliva',\n",
    "    'protesis_maxilar', 'protesis_maxilar_extension', 'protesis_mandibular', 'protesis_mandibular_extension',\n",
    "    'protesis_mandibular_material', 'satisfecho_protesis_sup', 'motivo_insatisfaccion_sup',\n",
    "    'satisfecho_protesis_inf', 'motivo_insatisfaccion_inf', 'n_protesis_sup_hechas', 'n_protesis_inf_hechas',\n",
    "    'usa_pasta_5000ppm', 'usa_colutorios', 'usa_pasta_1000ppm',\n",
    "    'ttm_dolor_mandibula_4sem', 'ttm_dolor_masticar_4sem', 'ttm_dolor_abrir_boca_4sem',\n",
    "    'ttm_mandibula_trabada_4sem', 'ttm_ruidos_articulacion_4sem', 'ttm_requiere_derivacion',\n",
    "    'eat10_perdida_peso', 'eat10_comer_fuera', 'eat10_tragar_liquidos', 'eat10_tragar_solidos',\n",
    "    'eat10_tragar_pastillas', 'eat10_tragar_doloroso', 'eat10_placer_comer', 'eat10_comida_pegada',\n",
    "    'eat10_tos_al_comer', 'eat10_tragar_estresante', 'eat10_requiere_derivacion'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844514dc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- Función de Carga y Limpieza de Pestañas (Generalizada) ---\n",
    "def load_and_clean_sheet(excel_path, sheet_name, clean_cols):\n",
    "    print(f\"  Cargando y limpiando pestaña '{sheet_name}'...\")\n",
    "    df = pd.read_excel(excel_path, sheet_name=sheet_name, header=1) # header=1 para fila 2\n",
    "    \n",
    "    # Eliminar la fila descriptiva 'NO MODIFICAR...' (ahora en la primera fila del DF)\n",
    "    df = df[~df[df.columns[0]].astype(str).str.contains('NO MODIFICAR', na=False)]\n",
    "    \n",
    "    # Asignar los nombres de columna limpios\n",
    "    if len(df.columns) != len(clean_cols):\n",
    "        print(f\"  ADVERTENCIA en {sheet_name}: Columnas leídas ({len(df.columns)}) no coinciden con nombres definidos ({len(clean_cols)}).\")\n",
    "        # Intentar renombrar solo las que coincidan por posición\n",
    "        new_names_map = {df.columns[i]: clean_cols[i] for i in range(min(len(df.columns), len(clean_cols)))}\n",
    "        df.rename(columns=new_names_map, inplace=True)\n",
    "    else:\n",
    "        df.columns = clean_cols\n",
    "    \n",
    "    # Filtrar filas donde 'correlativo' sea nulo y convertir a int\n",
    "    df.dropna(subset=['correlativo'], inplace=True)\n",
    "    df['correlativo'] = pd.to_numeric(df['correlativo'], errors='coerce').astype(int)\n",
    "    \n",
    "    # Convertir otras columnas a tipo numérico donde sea posible\n",
    "    for col in df.columns:\n",
    "        if col not in ['correlativo', 'otra_enfermedad_cual', 'farmacos_cuales', 'lesion_nombre_texto', 'motivo_no_termino_tto']:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    \n",
    "    print(f\"  Pestaña '{sheet_name}' limpia. {len(df)} registros.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c5a171",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# --- Función para Ejecutar y Evaluar un Modelo --- (Generalizada)\n",
    "def run_model_pipeline(X_data, y_data, model_type, target_name_for_report, output_prefix, use_smote=True):\n",
    "    print(f\"\\n--- Ejecutando Modelo {model_type} para {output_prefix} ---\")\n",
    "    \n",
    "    if len(np.unique(y_data)) < 2:\n",
    "        print(f\"  Error: Solo una clase en el target para {model_type}. Skip.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    # Chequeo desbalance\n",
    "    print(f\"  Target en {model_type}: {np.bincount(y_data)} (ratio hipo/normal: {sum(y_data==1)/sum(y_data==0):.2f})\")\n",
    "    \n",
    "    # Imputa ANTES de feature selection - mejorado\n",
    "    X_imputed = X_data.copy()\n",
    "    \n",
    "    # Replace infinite values with NaN first\n",
    "    X_imputed = X_imputed.replace([np.inf, -np.inf], np.nan)\n",
    "    \n",
    "    # Drop columns that are entirely NaN\n",
    "    X_imputed = X_imputed.dropna(axis=1, how='all')\n",
    "    \n",
    "    # For remaining NaNs, use mean imputation, but fill with 0 if mean is also NaN\n",
    "    for col in X_imputed.columns:\n",
    "        if X_imputed[col].isna().any():\n",
    "            mean_val = X_imputed[col].mean()\n",
    "            if pd.isna(mean_val):\n",
    "                X_imputed[col] = X_imputed[col].fillna(0)\n",
    "            else:\n",
    "                X_imputed[col] = X_imputed[col].fillna(mean_val)\n",
    "    \n",
    "    # Final check - replace any remaining NaNs with 0 es decir si no hay entonces asume 0\n",
    "    X_imputed = X_imputed.fillna(0)\n",
    "    \n",
    "    print(f\"  Después de limpieza: {X_imputed.shape[1]} columnas (eliminadas {X_data.shape[1] - X_imputed.shape[1]} columnas vacías)\")\n",
    "    \n",
    "    # Feature Selection pa' high-dim (top 20)\n",
    "    selector = SelectKBest(f_classif, k=min(20, X_imputed.shape[1]))\n",
    "    X_selected = selector.fit_transform(X_imputed, y_data)\n",
    "    selected_features = X_imputed.columns[selector.get_support()].tolist()\n",
    "    X_selected = pd.DataFrame(X_selected, columns=selected_features, index=X_imputed.index)\n",
    "    print(f\"  Features reducidas a {X_selected.shape[1]} (de {X_imputed.shape[1]})\")\n",
    "    \n",
    "    # Split\n",
    "    strat = True if len(y_data.unique()) > 1 and len(y_data) > 20 else False\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_selected, y_data, test_size=0.3, random_state=42, stratify=y_data if strat else None)\n",
    "    \n",
    "    # SMOTE pa' desbalance (solo train)\n",
    "    if use_smote and sum(y_train==1) > 0 and sum(y_train==0) > 0 and sum(y_train==1) < len(y_train):\n",
    "        smote = SMOTE(random_state=42, k_neighbors=min(3, sum(y_train==1)-1))\n",
    "        X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "        print(f\"  SMOTE: Balanceado train de {sum(y_train==1)}/{len(y_train)} hipos a {sum(y_train_res==1)}/{len(y_train_res)}\")\n",
    "    else:\n",
    "        X_train_res, y_train_res = X_train, y_train\n",
    "    \n",
    "    # Modelos (con weights)\n",
    "    if model_type == 'XGB':\n",
    "        pos_weight = sum(y_train_res==0) / sum(y_train_res==1) if sum(y_train_res==1) > 0 else 1\n",
    "        model = XGBClassifier(n_estimators=100, random_state=42, scale_pos_weight=pos_weight, eval_metric='logloss', use_label_encoder=False)\n",
    "    elif model_type == 'LR':\n",
    "        model = LogisticRegression(class_weight='balanced', random_state=42, max_iter=200)\n",
    "    elif model_type == 'SVM':\n",
    "        model = SVC(kernel='rbf', class_weight='balanced', probability=True, random_state=42)\n",
    "    elif model_type == 'RF':\n",
    "        model = RandomForestClassifier(n_estimators=100, class_weight='balanced', random_state=42)\n",
    "    else:\n",
    "        print(f\"  Error: Modelo {model_type} no soportado.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    model.fit(X_train_res, y_train_res)\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    auc = roc_auc_score(y_test, model.predict_proba(X_test)[:,1]) if hasattr(model, 'predict_proba') and len(np.unique(y_test)) > 1 else None\n",
    "    recall_hipo = f1_score(y_test, y_pred, average=None)[1] if len(np.unique(y_test)) > 1 and len(f1_score(y_test, y_pred, average=None)) > 1 else None # Recall para clase 1 (hipofunción)\n",
    "    \n",
    "    # CV pa' small N (LOO si <30, sino 5-fold)\n",
    "    cv_folds = LeaveOneOut() if len(y_data) < 30 else 5\n",
    "    cv_scores = cross_val_score(model, X_selected, y_data, cv=cv_folds, scoring='f1_weighted')\n",
    "    \n",
    "    # Fix AUC formatting\n",
    "    auc_str = f\"{auc:.4f}\" if auc is not None else \"N/A\"\n",
    "    recall_hipo_str = f\"{recall_hipo:.4f}\" if recall_hipo is not None else \"N/A\"\n",
    "    print(f\"  {model_type} (SMOTE={use_smote}) - Acc: {acc:.4f}, F1: {f1:.4f}, AUC: {auc_str}, CV F1: {cv_scores.mean():.4f} (+/- {cv_scores.std()*2:.4f}), Recall Hipo: {recall_hipo_str}\")\n",
    "    \n",
    "    # Report - Fix encoding issue\n",
    "    target_names = ['Normal (0)', 'Hipofuncion (1)'] if target_name_for_report == 'funcionalidad_oral_final' else ['Normal (0)', 'Reducida (1)']\n",
    "    print(f\"  Reporte de Clasificacion:\\n{classification_report(y_test, y_pred, target_names=target_names)}\")\n",
    "    \n",
    "    # CM\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Normal', 'Hipo'], yticklabels=['Normal', 'Hipo'])\n",
    "    plt.title(f'Matriz Confusión {model_type}')\n",
    "    plt.savefig(os.path.join(OUTPUT_DIR, f'cm_{output_prefix}_{model_type}.png'))\n",
    "    plt.clf()\n",
    "    \n",
    "    # Importancias\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        imp_df = pd.DataFrame({'feature': X_selected.columns, 'importance': model.feature_importances_}).sort_values('importance', ascending=False).head(20)\n",
    "        sns.barplot(x='importance', y='feature', data=imp_df)\n",
    "        plt.title(f'Top 20 Features {model_type}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'imp_{output_prefix}_{model_type}.png'))\n",
    "        plt.clf()\n",
    "        print(\"  Top 5:\", imp_df.head())\n",
    "    elif model_type in ['SVM', 'LR']:\n",
    "        # Permutation importance para SVM/LR\n",
    "        imp = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "        imp_df = pd.DataFrame({'feature': X_selected.columns, 'importance': imp.importances_mean}).sort_values('importance', ascending=False).head(20)\n",
    "        sns.barplot(x='importance', y='feature', data=imp_df)\n",
    "        plt.title(f'Top 20 Features {model_type} (Permutation)')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'imp_{output_prefix}_{model_type}.png'))\n",
    "        plt.clf()\n",
    "        print(\"  Top 5 (permutation):\", imp_df.head())\n",
    "    \n",
    "    # SHAP solo para XGB y RF\n",
    "    if model_type in ['XGB', 'RF']:\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        shap.summary_plot(shap_values, X_test, feature_names=X_selected.columns, show=False)\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'shap_{output_prefix}_{model_type}.png'))\n",
    "        plt.clf()\n",
    "        print(\"  SHAP guardado: Contribs por feature.\")\n",
    "    \n",
    "    return acc, f1, auc, recall_hipo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9162493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Script Principal: Orquestador del Análisis Completo ---\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- INICIANDO ANÁLISIS COMPLETO EDEPAM ---\")\n",
    "    excel_path_inicial = os.path.join(os.getcwd(), EXCEL_FILE_INICIAL)\n",
    "    excel_path_seguimiento = os.path.join(os.getcwd(), EXCEL_FILE_SEGUIMIENTO)\n",
    "    \n",
    "    if not os.path.exists(excel_path_inicial):\n",
    "        print(f\"ERROR: El archivo Excel '{excel_path_inicial}' no fue encontrado. Abortando.\")\n",
    "        import sys\n",
    "        sys.exit(1)\n",
    "    if not os.path.exists(excel_path_seguimiento):\n",
    "        print(f\"ERROR: El archivo Excel '{excel_path_seguimiento}' no fue encontrado. Abortando.\")\n",
    "        import sys\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        # --- PASO 1: Cargar y Limpiar Todas las Pestañas de Datos Iniciales ---\n",
    "        print(\"\\n[PASO 1/5] Cargando y Limpiando todas las pestañas de Base datos inicial.xlsx...\")\n",
    "        df_diag_ini = load_and_clean_sheet(excel_path_inicial, 'DIAGNOSTICO', clean_cols_diagnostico)\n",
    "        df_trasp_ini = load_and_clean_sheet(excel_path_inicial, 'Traspaso Cuestionario', clean_cols_traspaso)\n",
    "        df_cal_ini = load_and_clean_sheet(excel_path_inicial, 'Calidad de vida Basal', clean_cols_calidad_vida)\n",
    "        df_func_ini = load_and_clean_sheet(excel_path_inicial, 'Funcionalidad oral', clean_cols_funcionalidad_oral)\n",
    "        df_les_ini = load_and_clean_sheet(excel_path_inicial, 'Lesiones mucosa oral', clean_cols_lesiones_mucosa_oral)\n",
    "        df_copd_ini = load_and_clean_sheet(excel_path_inicial, 'COPD', clean_cols_copd)\n",
    "        print(\"\\nTodas las pestañas iniciales cargadas y limpiadas exitosamente.\")\n",
    "        \n",
    "        # <<< INICIO: Bloque para generar Heatmap de Correlación >>>\n",
    "        print(\"\\n[PASO ADICIONAL] Generando mapa de calor de correlaciones...\")\n",
    "        \n",
    "        # Unir los dataframes necesarios que contienen las columnas para el heatmap\n",
    "        # Merge df_func_ini con df_trasp_ini para obtener edad y sexo\n",
    "        df_for_heatmap = pd.merge(df_func_ini, df_trasp_ini[['correlativo', 'edad', 'sexo']], on='correlativo', how='left')\n",
    "        \n",
    "        # Seleccionar solo las columnas numéricas relevantes que realmente existen\n",
    "        cols_for_heatmap = ['edad', 'sexo', \n",
    "                           'leake_q1', 'leake_q2', 'leake_q3', 'leake_q4', 'leake_q5', \n",
    "                           'leake_resultado', 'eichner', 'funcion_masticatoria', \n",
    "                           'fuerza_oclusal', 'diadococinesia', 'funcion_deglutoria']\n",
    "        \n",
    "        # Verificar qué columnas realmente existen en el DataFrame\n",
    "        existing_cols = [col for col in cols_for_heatmap if col in df_for_heatmap.columns]\n",
    "        print(f\"  Columnas disponibles para heatmap: {existing_cols}\")\n",
    "        \n",
    "        if len(existing_cols) > 2:  # Solo generar si hay suficientes columnas\n",
    "            # Calcular la matriz de correlación\n",
    "            corr_matrix = df_for_heatmap[existing_cols].corr()\n",
    "            \n",
    "            # Generar el gráfico\n",
    "            plt.figure(figsize=(12, 10))\n",
    "            sns.heatmap(corr_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', linewidths=.5, center=0)\n",
    "            plt.title('Mapa de Calor de Correlación - Variables de Funcionalidad Oral', fontsize=14)\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.yticks(rotation=0)\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Guardar la figura\n",
    "            heatmap_path = os.path.join(OUTPUT_DIR, 'correlacion_heatmap.png')\n",
    "            plt.savefig(heatmap_path, dpi=300, bbox_inches='tight')\n",
    "            plt.clf()  # Limpiar la figura para los siguientes gráficos\n",
    "            print(f\"  Mapa de calor guardado en: {heatmap_path}\")\n",
    "        else:\n",
    "            print(\"  No hay suficientes columnas numéricas para generar el heatmap\")\n",
    "        # <<< FIN: Bloque para generar Heatmap de Correlación >>>\n",
    "        \n",
    "        # --- Feature Engineering (Datos Iniciales) ---\n",
    "        # BMI\n",
    "        df_trasp_ini['bmi'] = df_trasp_ini['peso_kg'] / ((df_trasp_ini['altura_cm']/100)**2)\n",
    "        # Sum caries (asumiendo 2=caries)\n",
    "        caries_cols_ini = [col for col in df_copd_ini.columns if 'diente_' in col]\n",
    "        df_copd_ini['sum_caries'] = df_copd_ini[caries_cols_ini].eq(2).sum(axis=1)\n",
    "        # Has lesion\n",
    "        df_les_ini['has_lesion'] = (df_les_ini['lesion_elemental'].notna()).astype(int)\n",
    "        \n",
    "        # --- Recalcular funcionalidad_oral_final para datos iniciales ---\n",
    "        df_func_ini['sum_func_components'] = df_func_ini['funcion_masticatoria'] + df_func_ini['fuerza_oclusal'] + df_func_ini['diadococinesia'] + df_func_ini['funcion_deglutoria']\n",
    "        df_diag_ini['funcionalidad_oral_final'] = (df_func_ini['sum_func_components'] >= 3).astype(int)\n",
    "        print(\"  'funcionalidad_oral_final' recalculada para datos iniciales.\")\n",
    "        \n",
    "        # --- PASO 2: Ejecución de Modelos de Machine Learning (Datos Iniciales) ---\n",
    "        print(\"\\n[PASO 2/5] Ejecutando Modelos de Machine Learning (Datos Iniciales)...\")\n",
    "        \n",
    "        # --- Preparación de datos para Modelos V1, V2, V3 ---\n",
    "        # V1: Solo Funcionalidad oral\n",
    "        df_v1 = df_func_ini.copy()\n",
    "        y_v1 = df_v1['funcion_masticatoria'].astype(int)\n",
    "        cols_to_drop_v1 = ['correlativo', 'leake_resultado', 'eichner', 'funcion_masticatoria', 'sum_func_components']\n",
    "        X_v1 = df_v1.select_dtypes(include=['float64', 'int64']).drop(columns=[c for c in cols_to_drop_v1 if c in df_v1.select_dtypes(include=['float64', 'int64']).columns])\n",
    "        \n",
    "        # V2: Funcionalidad oral + Traspaso\n",
    "        df_v2_combined = pd.merge(df_func_ini, df_trasp_ini, on='correlativo')\n",
    "        y_v2 = df_v2_combined['funcion_masticatoria'].astype(int)\n",
    "        cols_to_drop_v2 = ['correlativo', 'leake_resultado', 'eichner', 'funcion_masticatoria', 'sum_func_components', 'otra_enfermedad_cual', 'farmacos_cuales']\n",
    "        X_v2 = df_v2_combined.select_dtypes(include=['float64', 'int64']).drop(columns=[c for c in cols_to_drop_v2 if c in df_v2_combined.select_dtypes(include=['float64', 'int64']).columns])\n",
    "        \n",
    "        # V3: Funcionalidad oral + Traspaso + Calidad\n",
    "        df_v3_combined = pd.merge(df_func_ini, df_trasp_ini, on='correlativo')\n",
    "        df_v3_combined = pd.merge(df_v3_combined, df_cal_ini, on='correlativo')\n",
    "        y_v3 = df_v3_combined['funcion_masticatoria'].astype(int)\n",
    "        cols_to_drop_v3 = ['correlativo', 'leake_resultado', 'eichner', 'funcion_masticatoria', 'sum_func_components', 'otra_enfermedad_cual', 'farmacos_cuales']\n",
    "        X_v3 = df_v3_combined.select_dtypes(include=['float64', 'int64']).drop(columns=[c for c in cols_to_drop_v3 if c in df_v3_combined.select_dtypes(include=['float64', 'int64']).columns])\n",
    "        \n",
    "        # --- Ejecución de Modelos V1, V2, V3 con XGBoost ---\n",
    "        print(\"\\n--- Modelo V1 (Solo Funcionalidad Oral) - XGBoost ---\")\n",
    "        acc_v1_xgb, f1_v1_xgb, auc_v1_xgb, recall_v1_xgb = run_model_pipeline(X_v1, y_v1, 'XGB', 'funcion_masticatoria', 'v1_xgb', use_smote=True)\n",
    "        \n",
    "        print(\"\\n--- Modelo V2 (Funcionalidad Oral + Traspaso) - XGBoost ---\")\n",
    "        acc_v2_xgb, f1_v2_xgb, auc_v2_xgb, recall_v2_xgb = run_model_pipeline(X_v2, y_v2, 'XGB', 'funcion_masticatoria', 'v2_xgb', use_smote=True)\n",
    "        \n",
    "        print(\"\\n--- Modelo V3 (Funcionalidad Oral + Traspaso + Calidad) - XGBoost ---\")\n",
    "        acc_v3_xgb, f1_v3_xgb, auc_v3_xgb, recall_v3_xgb = run_model_pipeline(X_v3, y_v3, 'XGB', 'funcion_masticatoria', 'v3_xgb', use_smote=True)\n",
    "\n",
    "        # V4: Todos los datos basales\n",
    "        df_v4_combined = pd.merge(df_func_ini, df_trasp_ini, on='correlativo')\n",
    "        df_v4_combined = pd.merge(df_v4_combined, df_cal_ini, on='correlativo')\n",
    "        df_v4_combined = pd.merge(df_v4_combined, df_les_ini, on='correlativo')\n",
    "        df_v4_combined = pd.merge(df_v4_combined, df_copd_ini, on='correlativo')\n",
    "        df_v4_combined = pd.merge(df_v4_combined, df_diag_ini, on='correlativo')\n",
    "        \n",
    "        y_v4 = df_v4_combined['funcionalidad_oral_final'].astype(int)\n",
    "        cols_to_drop_v4 = [\n",
    "            'correlativo', 'leake_resultado', 'eichner', 'funcion_masticatoria', 'sum_func_components',\n",
    "            'funcionalidad_oral_final', 'otra_enfermedad_cual', 'farmacos_cuales', 'lesion_nombre_texto'\n",
    "        ]\n",
    "        X_v4 = df_v4_combined.select_dtypes(include=['float64', 'int64']).drop(columns=[c for c in cols_to_drop_v4 if c in df_v4_combined.select_dtypes(include=['float64', 'int64']).columns])\n",
    "        \n",
    "        # --- Ejecución de Modelos V4 --- (XGB, LR, SVM, RF)\n",
    "        print(\"\\n--- Modelo V4 (Todos los Datos Basales) ---\")\n",
    "        acc_v4_xgb, f1_v4_xgb, auc_v4_xgb, recall_v4_xgb = run_model_pipeline(X_v4, y_v4, 'XGB', 'funcionalidad_oral_final', 'v4_xgb', use_smote=True)\n",
    "        acc_v4_lr, f1_v4_lr, auc_v4_lr, recall_v4_lr = run_model_pipeline(X_v4, y_v4, 'LR', 'funcionalidad_oral_final', 'v4_lr', use_smote=True)\n",
    "        acc_v4_svm, f1_v4_svm, auc_v4_svm, recall_v4_svm = run_model_pipeline(X_v4, y_v4, 'SVM', 'funcionalidad_oral_final', 'v4_svm', use_smote=True)\n",
    "        acc_v4_rf, f1_v4_rf, auc_v4_rf, recall_v4_rf = run_model_pipeline(X_v4, y_v4, 'RF', 'funcionalidad_oral_final', 'v4_rf', use_smote=True)\n",
    "        \n",
    "        # Comparación V4\n",
    "        print(\"\\n--- Comparación Modelos V4 ---\")\n",
    "        auc_v4_xgb_str = f\"{auc_v4_xgb:.4f}\" if auc_v4_xgb else \"N/A\"\n",
    "        recall_v4_xgb_str = f\"{recall_v4_xgb:.4f}\" if recall_v4_xgb else \"N/A\"\n",
    "        auc_v4_lr_str = f\"{auc_v4_lr:.4f}\" if auc_v4_lr else \"N/A\"\n",
    "        recall_v4_lr_str = f\"{recall_v4_lr:.4f}\" if recall_v4_lr else \"N/A\"\n",
    "        auc_v4_svm_str = f\"{auc_v4_svm:.4f}\" if auc_v4_svm else \"N/A\"\n",
    "        recall_v4_svm_str = f\"{recall_v4_svm:.4f}\" if recall_v4_svm else \"N/A\"\n",
    "        auc_v4_rf_str = f\"{auc_v4_rf:.4f}\" if auc_v4_rf else \"N/A\"\n",
    "        recall_v4_rf_str = f\"{recall_v4_rf:.4f}\" if recall_v4_rf else \"N/A\"\n",
    "        \n",
    "        print(f\"XGB: Acc {acc_v4_xgb:.4f} | F1 {f1_v4_xgb:.4f} | AUC {auc_v4_xgb_str} | Recall Hipo {recall_v4_xgb_str}\")\n",
    "        print(f\"LR: Acc {acc_v4_lr:.4f} | F1 {f1_v4_lr:.4f} | AUC {auc_v4_lr_str} | Recall Hipo {recall_v4_lr_str}\")\n",
    "        print(f\"SVM: Acc {acc_v4_svm:.4f} | F1 {f1_v4_svm:.4f} | AUC {auc_v4_svm_str} | Recall Hipo {recall_v4_svm_str}\")\n",
    "        print(f\"RF: Acc {acc_v4_rf:.4f} | F1 {f1_v4_rf:.4f} | AUC {auc_v4_rf_str} | Recall Hipo {recall_v4_rf_str}\")\n",
    "        \n",
    "        print(\"\\nModelos de Machine Learning ejecutados exitosamente.\")\n",
    "        \n",
    "        # --- PASO 3: Cargar datos de seguimiento para comparación ---\n",
    "        print(\"\\n[PASO 3/5] Cargando datos de seguimiento...\")\n",
    "        df_diag_segu = load_and_clean_sheet(excel_path_seguimiento, 'DIAGNOSTICO', clean_cols_diagnostico)\n",
    "        df_func_segu = load_and_clean_sheet(excel_path_seguimiento, 'Funcionalidad oral', clean_cols_funcionalidad_oral)\n",
    "        # <<< INICIO: Carga de datos de Calidad de Vida de Seguimiento >>>\n",
    "        df_cal_segu = load_and_clean_sheet(excel_path_seguimiento, 'Calidad de vida Basal', clean_cols_calidad_vida)\n",
    "        # <<< FIN: Carga de datos de Calidad de Vida de Seguimiento >>>\n",
    "\n",
    "        # Recalcular funcionalidad_oral_final para seguimiento\n",
    "        df_func_segu['sum_func_components'] = df_func_segu['funcion_masticatoria'] + df_func_segu['fuerza_oclusal'] + df_func_segu['diadococinesia'] + df_func_segu['funcion_deglutoria']\n",
    "        df_diag_segu['funcionalidad_oral_final'] = (df_func_segu['sum_func_components'] >= 3).astype(int)\n",
    "        \n",
    "        # --- PASO 4: Análisis Comparativo ---\n",
    "        print(\"\\n[PASO 4/5] Ejecutando Análisis Comparativo...\")\n",
    "        \n",
    "        # Comparación pre/post\n",
    "        df_comparativo = pd.merge(df_diag_ini[['correlativo', 'funcionalidad_oral_final']], \n",
    "                                  df_diag_segu[['correlativo', 'funcionalidad_oral_final']], \n",
    "                                  on='correlativo', suffixes=('_ini', '_segu'))\n",
    "        \n",
    "        # Cambios func (heatmap)\n",
    "        cambios = pd.crosstab(df_comparativo['funcionalidad_oral_final_ini'], df_comparativo['funcionalidad_oral_final_segu'], margins=True)\n",
    "        print(\"\\nCambios Func Oral:\\n\", cambios)\n",
    "        sns.heatmap(cambios.iloc[:-1, :-1], annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Cambios Func Oral Pre/Post')\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, 'cambios_func.png'))\n",
    "        plt.clf()\n",
    "        \n",
    "        # McNemar test\n",
    "        mask_delta = df_comparativo['funcionalidad_oral_final_ini'].notna() & df_comparativo['funcionalidad_oral_final_segu'].notna()\n",
    "        if mask_delta.sum() > 1:\n",
    "            pre = df_comparativo.loc[mask_delta, 'funcionalidad_oral_final_ini'].astype(int)\n",
    "            post = df_comparativo.loc[mask_delta, 'funcionalidad_oral_final_segu'].astype(int)\n",
    "            \n",
    "            table = pd.crosstab(pre, post)\n",
    "            if table.shape == (2, 2):\n",
    "                if MCNEMAR_AVAILABLE:\n",
    "                    result = mcnemar(table, exact=False)\n",
    "                    print(f\"McNemar cambios func: p={result.pvalue:.4f}\")\n",
    "                else:\n",
    "                    chi2, p_value = mcnemar_simple(table)\n",
    "                    if p_value is not None:\n",
    "                        print(f\"McNemar cambios func: p={p_value:.4f}\")\n",
    "                    else:\n",
    "                        print(\"McNemar: No se pudo calcular\")\n",
    "        \n",
    "        # <<< INICIO: Bloque de Tests Estadísticos Adicionales >>>\n",
    "        print(\"\\n--- Tests Estadísticos Adicionales Pre/Post ---\")\n",
    "        \n",
    "        # 1. Comparación de Calidad de Vida (EQ-5D)\n",
    "        df_comp_calidad = pd.merge(df_cal_ini[['correlativo', 'eq5d_score_paciente']],\n",
    "                                   df_cal_segu[['correlativo', 'eq5d_score_paciente']],\n",
    "                                   on='correlativo', suffixes=('_ini', '_segu'))\n",
    "        df_comp_calidad.dropna(inplace=True)\n",
    "        if not df_comp_calidad.empty:\n",
    "            stat_wilcoxon, p_value_wilcoxon = wilcoxon(df_comp_calidad['eq5d_score_paciente_segu'], df_comp_calidad['eq5d_score_paciente_ini'])\n",
    "            print(f\"\\nAnálisis de Calidad de Vida (EQ-5D):\")\n",
    "            print(f\"  - Mediana Inicial: {df_comp_calidad['eq5d_score_paciente_ini'].median():.2f}\")\n",
    "            print(f\"  - Mediana Seguimiento: {df_comp_calidad['eq5d_score_paciente_segu'].median():.2f}\")\n",
    "            print(f\"  - Test de Wilcoxon: p-value = {p_value_wilcoxon:.4f}\")\n",
    "        \n",
    "        # 2. Comparación de Grado Periodontal\n",
    "        df_comp_perio = pd.merge(df_diag_ini[['correlativo', 'periodontal_grado']],\n",
    "                                 df_diag_segu[['correlativo', 'periodontal_grado']],\n",
    "                                 on='correlativo', suffixes=('_ini', '_segu'))\n",
    "        df_comp_perio.dropna(inplace=True)\n",
    "        if not df_comp_perio.empty:\n",
    "            stat_ttest, p_value_ttest = ttest_rel(df_comp_perio['periodontal_grado_segu'], df_comp_perio['periodontal_grado_ini'])\n",
    "            print(f\"\\nAnálisis de Grado Periodontal:\")\n",
    "            print(f\"  - Media Inicial: {df_comp_perio['periodontal_grado_ini'].mean():.2f}\")\n",
    "            print(f\"  - Media Seguimiento: {df_comp_perio['periodontal_grado_segu'].mean():.2f}\")\n",
    "            print(f\"  - Test T pareado: p-value = {p_value_ttest:.4f}\")\n",
    "        # <<< FIN: Bloque de Tests Estadísticos Adicionales >>>\n",
    "            \n",
    "        # --- PASO 5: Resumen Final ---\n",
    "        print(\"\\n[PASO 5/5] Resumen Final de Resultados:\")\n",
    "        print(f\"  Modelo V1 (Solo Funcionalidad Oral) - XGB Acc: {acc_v1_xgb:.4f}\")\n",
    "        print(f\"  Modelo V2 (Funcionalidad + Traspaso) - XGB Acc: {acc_v2_xgb:.4f}\")\n",
    "        print(f\"  Modelo V3 (Funcionalidad + Traspaso + Calidad) - XGB Acc: {acc_v3_xgb:.4f}\")\n",
    "        print(f\"  Modelo V4 (Todos los Datos) - XGB Acc: {acc_v4_xgb:.4f}, LR Acc: {acc_v4_lr:.4f}, SVM Acc: {acc_v4_svm:.4f}, RF Acc: {acc_v4_rf:.4f}\")\n",
    "        \n",
    "        # <<< INICIO: Bloque de Generación de Log de Métricas >>>\n",
    "        log_filepath = os.path.join(OUTPUT_DIR, \"resumen_metricas.txt\")\n",
    "        with open(log_filepath, \"w\", encoding='utf-8') as f:\n",
    "            f.write(\"--- Resumen de Metricas de Modelos V4 ---\\n\\n\")\n",
    "            f.write(f\"Random Forest (RF):\\n\")\n",
    "            f.write(f\"  - Accuracy: {acc_v4_rf:.4f}\\n\")\n",
    "            f.write(f\"  - F1-Score (Weighted): {f1_v4_rf:.4f}\\n\")\n",
    "            f.write(f\"  - AUC: {auc_v4_rf_str}\\n\")\n",
    "            f.write(f\"  - Recall Hipofuncion: {recall_v4_rf_str}\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Support Vector Machine (SVM):\\n\")\n",
    "            f.write(f\"  - Accuracy: {acc_v4_svm:.4f}\\n\")\n",
    "            f.write(f\"  - F1-Score (Weighted): {f1_v4_svm:.4f}\\n\")\n",
    "            f.write(f\"  - AUC: {auc_v4_svm_str}\\n\")\n",
    "            f.write(f\"  - Recall Hipofuncion: {recall_v4_svm_str}\\n\\n\")\n",
    "\n",
    "            f.write(f\"Logistic Regression (LR):\\n\")\n",
    "            f.write(f\"  - Accuracy: {acc_v4_lr:.4f}\\n\")\n",
    "            f.write(f\"  - F1-Score (Weighted): {f1_v4_lr:.4f}\\n\")\n",
    "            f.write(f\"  - AUC: {auc_v4_lr_str}\\n\")\n",
    "            f.write(f\"  - Recall Hipofuncion: {recall_v4_lr_str}\\n\\n\")\n",
    "\n",
    "            f.write(f\"XGBoost (XGB):\\n\")\n",
    "            f.write(f\"  - Accuracy: {acc_v4_xgb:.4f}\\n\")\n",
    "            f.write(f\"  - F1-Score (Weighted): {f1_v4_xgb:.4f}\\n\")\n",
    "            f.write(f\"  - AUC: {auc_v4_xgb_str}\\n\")\n",
    "            f.write(f\"  - Recall Hipofuncion: {recall_v4_xgb_str}\\n\\n\")\n",
    "            \n",
    "            f.write(\"\\n--- Resumen Tests Estadisticos Pre/Post ---\\n\\n\")\n",
    "            if 'p_value_wilcoxon' in locals():\n",
    "                f.write(f\"Calidad de Vida (EQ-5D) - Wilcoxon p-value: {p_value_wilcoxon:.4f}\\n\")\n",
    "            if 'p_value_ttest' in locals():\n",
    "                f.write(f\"Grado Periodontal - Paired t-test p-value: {p_value_ttest:.4f}\\n\")\n",
    "\n",
    "        print(f\"\\nLog de metricas guardado en: {log_filepath}\")\n",
    "        # <<< FIN: Bloque de Generación de Log de Métricas >>>\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error durante el análisis completo: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "    print(\"\\n--- ANÁLISIS COMPLETO EDEPAM FINALIZADO ---\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
